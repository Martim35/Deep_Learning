{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NXGSRTYF4rCXJmd6xupB0ABZezQgkIY1","timestamp":1680957882315}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Deep Learning Project - Pothole detection** <br/>\n","# **Ensemble Model**\n","**Data Science and Advanced Analytics with a specialization in Data Science**<br/>\n","**NOVA IMS**<br/>\n","Authors of this notebook:\n","*   Mafalda Paço - 20220619@novaims.unl.pt\n","*   Mª Margarida Graça - 20220602@novaims.unl.pt\n","*   Marta Dinis - 20220611@novaims.unl.pt\n","*   Nuno Dias - 20220603@novaims.unl.pt\n","*   Patrícia Morais - 20220638@novaims.unl.pt\n","\n","\n","## References\n","1.  https://www.v7labs.com/blog/ensemble-learning\n","2.  https://medium.com/randomai/ensemble-and-store-models-in-keras-2-x-b881a6d7693f\n","3.  https://builtin.com/machine-learning/ensemble-model"],"metadata":{"id":"OtfrSR0vpb01"}},{"cell_type":"markdown","source":["##Ready to use Dataset\n","https://drive.google.com/file/d/1KE507iE7Hwb7TiJINnvMYCXNIGrEgPvt/view?usp=share_link"],"metadata":{"id":"PMdqsp1DZXiN"}},{"cell_type":"markdown","source":["## **Summary**\n","\n","In this notebook you'll find an ensemble model, combining our handcrafted model, ResNet50 and VGG16. By combining the predictions of multiple models we have a chance of boosting accuracy. This approach tends to genenerate more robust predictions, anchoring itself on the wisdom of crowds.\n","All the choices made during the development of this model are detailed as we implemented them. You can also find the accuracy, AUROC and loss values for train and validation datasets.\n","We concluded that this model is worse than the one we handcrafted."],"metadata":{"id":"kY_H_XNQpYov"}},{"cell_type":"markdown","source":["## **Data Import**"],"metadata":{"id":"vPhvX_NIpn_R"}},{"cell_type":"markdown","source":["Necessary library imports."],"metadata":{"id":"kfW3lAqfpquw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lCQwAR6ju1t","executionInfo":{"status":"ok","timestamp":1680966909113,"user_tz":-60,"elapsed":14425,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"be573d70-d9d1-4e25-f3ce-e8bd5c5f31e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.2/172.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U keras-tuner\n","\n","import os\n","import numpy as np\n","from PIL import Image\n","\n","import time\n","import shutil\n","import zipfile\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras import Sequential, Model, layers, initializers, regularizers, optimizers, metrics\n","\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D, Input\n","from tensorflow.keras.models import Model\n","\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.applications import ResNet50, VGG16\n","from tensorflow.keras.optimizers import Adam\n","\n","import cv2\n","from sklearn.metrics import roc_auc_score\n","from tensorflow.keras.losses import binary_crossentropy"]},{"cell_type":"markdown","metadata":{"id":"SRF72xnnWM3n"},"source":["Connection to the Data Source."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41359,"status":"ok","timestamp":1680966955074,"user":{"displayName":"margarida graça","userId":"03894232038435072752"},"user_tz":-60},"id":"QSBFM5KQjiaW","outputId":"9b05d5a4-9718-4a49-c770-10cc95bbe95f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","File transfer completed in 6.447 seconds\n"]}],"source":["# Set the machine\n","gdrive = True\n","# Set the connection string\n","path = \"/content/drive/MyDrive/DL/Project/\"\n","main_folder, training_folder, testing_folder = \"DATA/\", \"train/\", \"test/\"\n","# If using Google Drive\n","if gdrive:\n","    # Setup drive\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Transfer zip dataset to the current virtual machine\n","    t0 = time.time()\n","    shutil.copyfile(path + 'DATA.zip', 'DATA.zip')\n","    # Extract files\n","    zip_ = zipfile.ZipFile('DATA.zip')\n","    zip_.extractall()\n","    zip_.close()\n","    print(\"File transfer completed in %0.3f seconds\" % (time.time() - t0))\n","    path = \"\""]},{"cell_type":"markdown","metadata":{"id":"A9ocz0SZkwm4"},"source":["Definition of a list of parameters for the function image_dataset_from_directory. We defined the size to which all images will be resized as well as the the number of batches at a time that our model will be trained on. All of these parameters were adapted accordingly to our problem's complexity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YH9pilg5KQFP"},"outputs":[],"source":["image_size=(224, 224) # Experimetar\n","crop_to_aspect_ratio=True # Experimentar\n","color_mode='rgb'\n","batch_size=64\n","label_mode=\"binary\"\n","validation_split=0.2\n","shuffle=True\n","seed=0"]},{"cell_type":"markdown","metadata":{"id":"dCVifK6DXSFH"},"source":["Loads the training data using the ``image_dataset_from_directory()``function and does an automatic split between training and validation data via validation_split, saving 20% for Validation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3996,"status":"ok","timestamp":1680967015180,"user":{"displayName":"margarida graça","userId":"03894232038435072752"},"user_tz":-60},"id":"hZVYAFbfKW8C","outputId":"4919f604-0913-4d01-9608-a8c3acf8aaf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1436 files belonging to 2 classes.\n","Using 1149 files for training.\n","Using 287 files for validation.\n"]}],"source":["# Generate an object of type tf.data.Dataset\n","ds_train, ds_val = image_dataset_from_directory(path + main_folder + training_folder,\n","                                                image_size=image_size,\n","                                                crop_to_aspect_ratio=crop_to_aspect_ratio,\n","                                                color_mode=color_mode,\n","                                                batch_size=batch_size,\n","                                                label_mode=label_mode,\n","                                                subset='both',\n","                                                validation_split=validation_split,\n","                                                shuffle=shuffle,\n","                                                seed=seed)"]},{"cell_type":"markdown","metadata":{"id":"HMOzVmnhXZnj"},"source":["Loads the testing data using the ``image_dataset_from_directory()``function."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1680967016940,"user":{"displayName":"margarida graça","userId":"03894232038435072752"},"user_tz":-60},"id":"ZRTkz6xeKXk0","outputId":"4f6c172a-73ef-448a-8dd0-4999b395d1d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 16 files belonging to 2 classes.\n"]}],"source":["ds_test = image_dataset_from_directory(path + main_folder + testing_folder,\n","                                       image_size=image_size,\n","                                       crop_to_aspect_ratio=crop_to_aspect_ratio,\n","                                       color_mode=color_mode,\n","                                       batch_size=batch_size,\n","                                       label_mode=label_mode,\n","                                       shuffle=shuffle,\n","                                       seed=seed)"]},{"cell_type":"code","source":["input_shape=(*image_size, 3)\n","input_shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yL2bkhhNe7i_","executionInfo":{"status":"ok","timestamp":1680967019848,"user_tz":-60,"elapsed":8,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"ee0a6be6-afc2-4b2c-9d5a-118fddd80a16"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(224, 224, 3)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"RuS-4W3byJ6E"},"source":["With this preprocess function we are flattening and normalizing our image data, in order to make it suitable to train a machine learning model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iC2xx5sgcZv4"},"outputs":[],"source":["def preprocess(ds):\n","    X = []\n","    y = []\n","    for images, labels in ds:\n","        # Flatten images\n","        flat_images = images.numpy().reshape(images.shape[0], -1)\n","        X.extend(flat_images)\n","\n","        # Get labels\n","        y.extend(labels.numpy())\n","\n","    # Normalize images\n","    X = np.array(X).astype('float32') / 255.0\n","    y = np.array(y)\n","\n","    return X, y\n","\n","X_train, y_train = preprocess(ds_train)\n","X_val, y_val = preprocess(ds_val)"]},{"cell_type":"markdown","source":["This cell guarantees that the images are compatible with the pre-trained models which expect input images of size (224, 224) with 3 color channels."],"metadata":{"id":"bD45m3LgXzKy"}},{"cell_type":"code","source":["# Load and resize the input images to (224, 224)\n","X_train_resized = []\n","X_val_resized = []\n","\n","for img in X_train:\n","    img_resized = cv2.resize(img, (224, 224))\n","    X_train_resized.append(img_resized)\n","\n","for img in X_val:\n","    img_resized = cv2.resize(img, (224, 224))\n","    X_val_resized.append(img_resized)\n","\n","X_train_resized = np.array(X_train_resized)\n","X_val_resized = np.array(X_val_resized)\n","\n","# Ensure the input data has the correct number of channels\n","if X_train_resized.ndim == 3:\n","    X_train_resized = np.expand_dims(X_train_resized, axis=-1)\n","    X_train_resized = np.repeat(X_train_resized, 3, axis=-1)\n","\n","if X_val_resized.ndim == 3:\n","    X_val_resized = np.expand_dims(X_val_resized, axis=-1)\n","    X_val_resized = np.repeat(X_val_resized, 3, axis=-1)"],"metadata":{"id":"WGj7-BsdN_SW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Models**\n"],"metadata":{"id":"vRgMP0XQ3yrM"}},{"cell_type":"markdown","source":["###ResNet50"],"metadata":{"id":"z877d3mSeJSL"}},{"cell_type":"markdown","source":["ResNet stands for Residual Network, and is a 50 layer convolutional neural network (CNN). It uses shortcut connections, skipping over some convolutional layers and bypassing the vanishing gradient problem. (After a certain amount of backpropagation the gradients become so small that the model's weights cannot change, leading to the earlier layers of the network to not learn effectively). By using these residual blocks, the model is able to learn more complex representations of the input images."],"metadata":{"id":"r13_ushl1G-q"}},{"cell_type":"code","source":["# Load pre-trained ResNet50 model\n","resnet50 = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n","resnet50.trainable = False"],"metadata":{"id":"3NwHfcwXOJJn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967055667,"user_tz":-60,"elapsed":9523,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"ef8db93c-2dfb-4c33-eaf3-b7884aea8f24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94765736/94765736 [==============================] - 6s 0us/step\n"]}]},{"cell_type":"markdown","source":["###VGG16"],"metadata":{"id":"zch7g2eiOL8_"}},{"cell_type":"markdown","source":["VGG is a convolutional neural network comprised of 16 layers. It uses a 3x3 filter, the smallest possible size to capture spatial features. It's a more complex model than ResNet, having more filters and relying on increasing the number of layers to improve accuracy. By using many filters with samll receptive fields it's able to capture fine-grained features in the input images."],"metadata":{"id":"f-19VHA2OLvX"}},{"cell_type":"code","source":["# Load pre-trained VGG16 model\n","vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n","vgg16.trainable = False"],"metadata":{"id":"rs7Ybqo0OMec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967060321,"user_tz":-60,"elapsed":4662,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"cd762820-c8f9-4d71-b5eb-ed5810bf27de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 4s 0us/step\n"]}]},{"cell_type":"markdown","source":["###Our BEST MODEL"],"metadata":{"id":"obJf2iEqORCl"}},{"cell_type":"code","source":["# Define the custom model\n","class SimpleCNN_DO_L2_ES_3CL_BN(Model):\n","    def __init__(self, seed=0):\n","        super().__init__()\n","        self.preprocess = tf.keras.layers.BatchNormalization()\n","        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu',\n","                                           kernel_initializer=tf.keras.initializers.GlorotNormal(seed=seed))\n","        self.bn1 = tf.keras.layers.BatchNormalization()\n","        self.conv2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu',\n","                                           kernel_initializer=tf.keras.initializers.GlorotNormal(seed=seed))\n","        self.bn2 = tf.keras.layers.BatchNormalization()\n","        self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu',\n","                                           kernel_initializer=tf.keras.initializers.GlorotNormal(seed=seed))\n","        self.bn3 = tf.keras.layers.BatchNormalization()\n","        self.dense1 = tf.keras.layers.Dense(units=1, activation='sigmoid',\n","                                            kernel_initializer=tf.keras.initializers.GlorotNormal(seed=seed),\n","                                            kernel_regularizer=tf.keras.regularizers.l2(0.01))  # Add L2 regularization\n","        self.dropout1 = tf.keras.layers.Dropout(0.15, seed=seed)  # Add dropout layer\n","\n","        # Non-learnable layers (define only once)\n","        self.gmp = tf.keras.layers.GlobalMaxPooling2D()\n","        self.maxpool2x2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n","\n","    def call(self, inputs):\n","        x = self.preprocess(inputs)\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.maxpool2x2(x)\n","        x = self.dropout1(x)  # Apply dropout to the output of conv1\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.maxpool2x2(x)\n","\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.gmp(x)\n","        x = self.dense1(x)\n","        return x\n","\n","# Create an instance of the custom model\n","custom_model = SimpleCNN_DO_L2_ES_3CL_BN()"],"metadata":{"id":"2zn0dTZ7ORcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We pre-trained ResNet50, VGG16, and our custom model, we processed training images and extracted features with a diverse range of techniques. These features were then merged to shape the final ensemble's dataset."],"metadata":{"id":"oSfXenypQmeU"}},{"cell_type":"code","source":["# Extract features from the pre-trained models\n","resnet50_features = resnet50.predict(X_train_resized)\n","vgg16_features = vgg16.predict(X_train_resized)\n","custom_model_features = custom_model.predict(X_train_resized)\n","\n","# Flatten the features\n","resnet50_features_flat = resnet50_features.reshape((resnet50_features.shape[0], -1))\n","vgg16_features_flat = vgg16_features.reshape((vgg16_features.shape[0], -1))\n","custom_model_features_flat = custom_model_features.reshape((custom_model_features.shape[0], -1))\n","\n","# Concatenate the features\n","ensemble_features = np.concatenate([resnet50_features_flat, vgg16_features_flat, custom_model_features_flat], axis=1)\n"],"metadata":{"id":"7a6h_v4eOxkh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967103011,"user_tz":-60,"elapsed":42699,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"a3207461-4397-4cea-9686-e6e2a0a22787"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["36/36 [==============================] - 14s 111ms/step\n","36/36 [==============================] - 12s 212ms/step\n","36/36 [==============================] - 2s 24ms/step\n"]}]},{"cell_type":"markdown","source":["We then created a simple classifier with one dense layer and a sigmoid activation function. We compiled and trained the ensemble classifier using the combined features from the pre-trained models and corresponding labels."],"metadata":{"id":"L9iOvZFnQwrn"}},{"cell_type":"code","source":["# Create the final classifier\n","classifier = tf.keras.Sequential()\n","classifier.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","# Compile the classifier\n","classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the classifier\n","classifier.fit(ensemble_features, y_train, epochs=10, batch_size=32)\n"],"metadata":{"id":"SPpGtzUSPfSA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967114827,"user_tz":-60,"elapsed":11824,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"5c1264b0-18eb-40b9-c4e6-cf78db338504"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","36/36 [==============================] - 2s 12ms/step - loss: 1.6757 - accuracy: 0.5718\n","Epoch 2/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.5762 - accuracy: 0.7485\n","Epoch 3/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.4642 - accuracy: 0.7868\n","Epoch 4/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.5020 - accuracy: 0.7554\n","Epoch 5/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.4326 - accuracy: 0.8155\n","Epoch 6/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.4192 - accuracy: 0.8068\n","Epoch 7/10\n","36/36 [==============================] - 0s 12ms/step - loss: 0.4263 - accuracy: 0.8033\n","Epoch 8/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.4894 - accuracy: 0.7842\n","Epoch 9/10\n","36/36 [==============================] - 0s 11ms/step - loss: 0.6454 - accuracy: 0.7537\n","Epoch 10/10\n","36/36 [==============================] - 0s 10ms/step - loss: 0.3994 - accuracy: 0.8312\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f653ce87400>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["We extracted the features from the pre-trained models for validation data, flattened these features, and then concatenated them so they would be the same size."],"metadata":{"id":"f64rUTjwRK7Z"}},{"cell_type":"code","source":["# Extract features from the pre-trained models for validation data\n","resnet50_features_val = resnet50.predict(X_val_resized)\n","vgg16_features_val = vgg16.predict(X_val_resized)\n","custom_model_features_val = custom_model.predict(X_val_resized)\n","\n","# Flatten the features\n","resnet50_features_val_flat = resnet50_features_val.reshape((resnet50_features_val.shape[0], -1))\n","vgg16_features_val_flat = vgg16_features_val.reshape((vgg16_features_val.shape[0], -1))\n","custom_model_features_val_flat = custom_model_features_val.reshape((custom_model_features_val.shape[0], -1))\n","\n","# Concatenate the features\n","ensemble_features_val = np.concatenate([resnet50_features_val_flat, vgg16_features_val_flat, custom_model_features_val_flat], axis=1)"],"metadata":{"id":"_Sg2xtsfk-Cj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967124231,"user_tz":-60,"elapsed":9410,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"5994ae4e-95af-40a2-f4d8-3cc1b3a482f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["9/9 [==============================] - 2s 223ms/step\n","9/9 [==============================] - 5s 612ms/step\n","9/9 [==============================] - 0s 52ms/step\n"]}]},{"cell_type":"markdown","source":["Finally we printed the accuracy, AUROC, and loss for both training and validation data."],"metadata":{"id":"TjzWXasIRV6B"}},{"cell_type":"code","source":["# Make predictions with the classifier on training data\n","y_pred_train = classifier.predict(ensemble_features)\n","y_pred = classifier.predict(ensemble_features_val)\n","\n","# Calculate the accuracy for train and validation data\n","train_accuracy = accuracy_score(y_train, y_pred_train.round())\n","val_accuracy = accuracy_score(y_val, y_pred.round())\n","\n","# Calculate the AUROC score for train and validation data\n","train_auroc = roc_auc_score(y_train, y_pred_train)\n","val_auroc = roc_auc_score(y_val, y_pred)\n","\n","# Calculate the loss for train and validation data\n","train_loss = binary_crossentropy(y_train, y_pred_train).numpy().mean()\n","val_loss = binary_crossentropy(y_val, y_pred).numpy().mean()\n","\n","# Print the evaluation metrics for train data\n","print(\"Training Data Metrics:\")\n","print(\"Accuracy:\", train_accuracy)\n","print(\"AUROC:\", train_auroc)\n","print(\"Loss:\", train_loss)\n","print()\n","\n","# Print the evaluation metrics for validation data\n","print(\"Validation Data Metrics:\")\n","print(\"Accuracy:\", val_accuracy)\n","print(\"AUROC:\", val_auroc)\n","print(\"Loss:\", val_loss)"],"metadata":{"id":"geJHBsR96Ubu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680967346831,"user_tz":-60,"elapsed":1194,"user":{"displayName":"margarida graça","userId":"03894232038435072752"}},"outputId":"298d6fea-a189-4690-db9d-d874ad804986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["36/36 [==============================] - 0s 8ms/step\n","9/9 [==============================] - 0s 9ms/step\n","Training Data Metrics:\n","Accuracy: 0.8268059181897301\n","AUROC: 0.9417990540868301\n","Loss: 0.37872666\n","\n","Validation Data Metrics:\n","Accuracy: 0.7735191637630662\n","AUROC: 0.8951824817518248\n","Loss: 0.5018571\n"]}]},{"cell_type":"markdown","source":["### **Conclusion**\n","\n","We couldn't reach a satisfactory result, since the accuracy is worse than other models we tested and some overfitting, even though ensemble is a good technique\n","It might be possible to get better results if we used other models in the ensemble."],"metadata":{"id":"h03KtICM35Qx"}}]}