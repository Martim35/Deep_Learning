{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp3nnL0R2Jlt"
   },
   "source": [
    "# **Deep Learning Project - Pothole detection** <br/>\n",
    "# **Preprocess**\n",
    "**Data Science and Advanced Analytics with a specialization in Data Science**<br/>\n",
    "**NOVA IMS**<br/>\n",
    "Authors of this notebook: <br/>\n",
    "* Mafalda Paço (20220619@novaims.unl.pt)<br/>\n",
    "* Mª Margarida Graça (20220602@novaims.unl.pt)<br/>\n",
    "* Marta Dinis (20220611@novaims.unl.pt)<br/>\n",
    "* Nuno Dias (20220603@novaims.unl.pt)<br/>\n",
    "* Patrícia Morais (20220638@novaims.unl.pt)<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smZfVLMBihcn"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b8Cz7IGihfz"
   },
   "source": [
    "## Ready to use Dataset\n",
    "https://drive.google.com/file/d/1KE507iE7Hwb7TiJINnvMYCXNIGrEgPvt/view?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsXa_8IIs070"
   },
   "source": [
    "## **Summary**\n",
    "In this notebook, we did the necessary preprocess in our dataset.\n",
    "We deleted the found duplicates in the Data Exploration notebook and converted all the images to the same format and color mode. Lastly, we apllied data augmentation on all images and saved them (old and augmented) in a new folder, DATA.zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfmiLu6taaEu"
   },
   "source": [
    "## **Data Import**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA8OQ1PRWFHx"
   },
   "source": [
    "Necessary library imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqtMe2sc9PTL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xb9uM8mH_0MU",
    "outputId": "969d0ab4-97a2-4b8e-8b76-959c1490b7ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "File transfer completed in 7.903 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set the machine\n",
    "gdrive = True\n",
    "# Set the connection string\n",
    "path = \"/content/drive/MyDrive/DL/Project/\"\n",
    "main_folder, training_folder, testing_folder = \"DATA_original/\", \"train/\", \"test/\"\n",
    "# If using Google Drive\n",
    "if gdrive:\n",
    "    # Setup drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Transfer zip dataset to the current virtual machine\n",
    "    t0 = time.time()\n",
    "    shutil.copyfile(path + 'DATA_original.zip', 'DATA_original.zip')\n",
    "    # Extract files\n",
    "    zip_ = zipfile.ZipFile('DATA_original.zip')\n",
    "    zip_.extractall()\n",
    "    zip_.close()\n",
    "    print(\"File transfer completed in %0.3f seconds\" % (time.time() - t0))\n",
    "    path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_Oku0F_w2DF"
   },
   "source": [
    "After doing Data Exploration we noticed we have some duplicates in our dataset, this function removes them, creating a folder without duplicates in our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjts2rcecqg0"
   },
   "outputs": [],
   "source": [
    "def get_all_image_paths(folder_path):\n",
    "    image_paths = []\n",
    "    subfolders = os.listdir(folder_path)\n",
    "    for subfolder in subfolders:\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                    image_paths.append(os.path.join(subfolder_path, filename))\n",
    "    return image_paths\n",
    "\n",
    "def remove_duplicates_and_save(input_folder, output_folder):\n",
    "    image_paths = get_all_image_paths(input_folder)\n",
    "    image_hashes = {}\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        with Image.open(image_path) as img:\n",
    "            image_hash = str(hash(img.tobytes()))\n",
    "        if image_hash not in image_hashes:\n",
    "            image_hashes[image_hash] = image_path\n",
    "\n",
    "            # Create subfolder if it doesn't exist\n",
    "            subfolder = os.path.dirname(image_path).split('/')[-1]\n",
    "            new_subfolder_path = os.path.join(output_folder, subfolder)\n",
    "            os.makedirs(new_subfolder_path, exist_ok=True)\n",
    "\n",
    "            # Copy the image to the new folder\n",
    "            new_image_path = os.path.join(new_subfolder_path, os.path.basename(image_path))\n",
    "            shutil.copy(image_path, new_image_path)\n",
    "\n",
    "# Set the input and output paths\n",
    "input_base_folder = os.path.join('/content/', \"DATA_original\")\n",
    "path = \"/content/drive/MyDrive/DL/Project/\"\n",
    "output_base_folder = os.path.join(path, \"DATA_no_DUPLICATES_1\")\n",
    "\n",
    "# Remove duplicates and save the results for train and test folders\n",
    "for folder in [\"train\", \"test\"]:\n",
    "    input_folder = os.path.join(input_base_folder, folder)\n",
    "    output_folder = os.path.join(output_base_folder, folder)\n",
    "    remove_duplicates_and_save(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G4VWAVTyuoY"
   },
   "source": [
    "Now that we have removed the duplicates, we will format all images to '.jpeg' and 'RGB' so we can use them in our model without erorrs, creating a new folder with images in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKwp7IVdh1fG",
    "outputId": "3a475aff-476e-49bc-c4e2-28f396833258"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/PIL/Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_all_image_paths(folder_path):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "def convert_image_to_jpeg_rgb(image_path, output_path):\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img.save(output_path, format='JPEG')\n",
    "\n",
    "input_path = \"/content/drive/MyDrive/DL/Project/DATA_no_DUPLICATES_1\"\n",
    "output_path = \"/content/drive/MyDrive/DL/Project/DATA_FORMATTED_2\"\n",
    "\n",
    "# Create the output folder\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Get all image paths\n",
    "image_paths = get_all_image_paths(input_path)\n",
    "\n",
    "# Convert images and save them in the output folder\n",
    "for image_path in image_paths:\n",
    "    relative_path = os.path.relpath(image_path, input_path)\n",
    "    new_output_path = os.path.join(output_path, relative_path)\n",
    "    new_output_path = os.path.splitext(new_output_path)[0] + '.jpeg'\n",
    "    os.makedirs(os.path.dirname(new_output_path), exist_ok=True)\n",
    "    convert_image_to_jpeg_rgb(image_path, new_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVx9D0Hyz-6B"
   },
   "source": [
    "Since we have a small dataset we use data augmentation on all images, since it also helps to improve the performance and reduce overfiting, putting all images in a new '.zip' called DATA ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLYT0xC8lGtG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def get_all_image_paths(folder_path):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "def apply_augmentation(image_path, output_path, augmentation_pipeline):\n",
    "    image = load_img(image_path)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array = tf.expand_dims(image_array, 0)\n",
    "\n",
    "    augmented_image = augmentation_pipeline(image_array)\n",
    "    augmented_image = tf.squeeze(augmented_image, 0).numpy().astype(\"uint8\")\n",
    "\n",
    "    img = Image.fromarray(augmented_image)\n",
    "    img.save(output_path, format='JPEG')\n",
    "\n",
    "input_path = \"/content/drive/MyDrive/DL/Project/DATA_FORMATTED_2\"\n",
    "output_folder = \"/content/drive/MyDrive/DL/Project/DATA\"\n",
    "\n",
    "# Create the output folder for training set\n",
    "train_output_folder = os.path.join(output_folder, \"train\")\n",
    "os.makedirs(train_output_folder, exist_ok=True)\n",
    "\n",
    "# Create the output folder for test set\n",
    "test_output_folder = os.path.join(output_folder, \"test\")\n",
    "os.makedirs(test_output_folder, exist_ok=True)\n",
    "\n",
    "# Get all image paths\n",
    "train_image_paths = get_all_image_paths(os.path.join(input_path, \"train\"))\n",
    "test_image_paths = get_all_image_paths(os.path.join(input_path, \"test\"))\n",
    "\n",
    "# Shuffle the image paths\n",
    "random.shuffle(train_image_paths)\n",
    "random.shuffle(test_image_paths)\n",
    "\n",
    "# Define augmentation pipeline for training set\n",
    "augmentation_pipeline = tf.keras.Sequential([\n",
    "    layers.RandomFlip(),\n",
    "    layers.RandomRotation(factor=0.2),\n",
    "    layers.RandomZoom(height_factor=0.1, width_factor=0.1),\n",
    "    layers.RandomContrast(factor=0.25),\n",
    "    layers.RandomBrightness(factor=0.2),\n",
    "    layers.RandomTranslation(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1))\n",
    "], name=\"my_augmentation_pipeline\")\n",
    "\n",
    "# Copy and augment training images\n",
    "for image_path in train_image_paths:\n",
    "    relative_path = os.path.relpath(image_path, os.path.join(input_path, \"train\"))\n",
    "    new_output_path = os.path.join(train_output_folder, relative_path)\n",
    "\n",
    "    os.makedirs(os.path.dirname(new_output_path), exist_ok=True)\n",
    "\n",
    "    # Copy the original image\n",
    "    shutil.copyfile(image_path, new_output_path)\n",
    "\n",
    "    # Apply data augmentation to the training images\n",
    "    new_augmented_path = os.path.splitext(new_output_path)[0] + '_augmented.jpeg'\n",
    "    apply_augmentation(image_path, new_augmented_path, augmentation_pipeline)\n",
    "\n",
    "# Copy test images without augmentation\n",
    "for image_path in test_image_paths:\n",
    "    relative_path = os.path.relpath(image_path, os.path.join(input_path, \"test\"))\n",
    "    new_output_path = os.path.join(test_output_folder, relative_path)\n",
    "\n",
    "    os.makedirs(os.path.dirname(new_output_path), exist_ok=True)\n",
    "\n",
    "    # Copy the original image\n",
    "    shutil.copyfile(image_path, new_output_path)\n",
    "\n",
    "# Create a zip file\n",
    "with zipfile.ZipFile('/content/drive/MyDrive/DL/Project/DATA.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for folder_name, _, filenames in os.walk(output_folder):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(folder_name, filename)\n",
    "            zf.write(file_path, os.path.join(\"DATA\", os.path.relpath(file_path, output_folder)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
